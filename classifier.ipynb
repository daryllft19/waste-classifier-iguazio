{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code from /User/waste-classifier/src-tf will be used to train on the v3io:///projects/{{run.project}}/artifacts/images given dataset\n"
     ]
    }
   ],
   "source": [
    "import nuclio\n",
    "from os import environ, path\n",
    "from mlrun import mlconf\n",
    "\n",
    "# Define the `dbpath` - Where our `mlrun-api` services is available\n",
    "# the `http://mlrun-api:8080` is auto-defined by `kubernetes service` definition \n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "# Define the artifact path for the current user's home directory\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{environ[\"HOME\"]}/artifacts'\n",
    "\n",
    "# # specify paths and artifacts target location\n",
    "code_dir = path.join(path.abspath('./'), 'src-tf') # Where our source code files are saved\n",
    "images_path = path.join(mlconf.artifact_path, 'images') \n",
    "\n",
    "# Specify the project's name for experiment tracking\n",
    "project_name='waste-classifier'\n",
    "\n",
    "print(f'Code from {code_dir} will be used to train on the {images_path} given dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Download and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlrun import DataItem\n",
    " \n",
    "def open_archive(context, \n",
    "                 archive_url: DataItem,\n",
    "                 target_path,\n",
    "                 refresh=False\n",
    "                ):\n",
    "    \"\"\"Open a file/object archive into a target directory\n",
    "    \n",
    "    Currently supports zip and tar.gz\n",
    "    \n",
    "    :param context:      function execution context\n",
    "    :param archive_url:  url of archive file\n",
    "    :param target_path:  file system path to store extracted files\n",
    "    :param key:          key of archive contents in artifact store\n",
    "    :param train_size:    set the train dataset size out of total dataset\n",
    "    \"\"\"\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    \n",
    "    # get the archive as a local file (download if needed)\n",
    "    archive_url = archive_url.local()\n",
    "    \n",
    "    context.logger.info('Extracting zip')\n",
    "    extraction_path = os.path.join(target_path, 'tmp')\n",
    "    zip_ref = zipfile.ZipFile(archive_url, 'r')\n",
    "    zip_ref.extractall(extraction_path)\n",
    "    \n",
    "    \n",
    "    for data_train_test_type in ['TRAIN', 'TEST']:\n",
    "        context.logger.info(f'Processing {data_train_test_type} files')\n",
    "        # get all files paths from `extraction_path`\n",
    "        filenames = [file for file in glob(extraction_path + f'/DATASET/{data_train_test_type}/*/*') if file.endswith('.jpg')]\n",
    "        context.logger.info(f'{len(filenames)} files in {data_train_test_type}')\n",
    "        \n",
    "        # extract labels from filenames by their naming convention (<label>.<number>.jpg)\n",
    "        # and calculate how many images we have per label\n",
    "        context.logger.info('Extracting labels')\n",
    "        _extract_label = lambda filename: os.path.basename(filename).split('_')[0]\n",
    "        file_labels = [_extract_label(file) for file in filenames]\n",
    "        labels, label_counts = np.unique(file_labels, return_counts=True)\n",
    "\n",
    "        # Order the files into a {<label>: [<files list>]} dictionary\n",
    "        context.logger.info('Adding filenames in a dictionary')\n",
    "        files = {label: [] for label in labels}\n",
    "        for label, file in zip(file_labels, filenames):\n",
    "            files[label].append(file)\n",
    "\n",
    "        # create directories for train and test\n",
    "        for label in labels:\n",
    "            _dir = os.path.join(target_path, data_train_test_type, label)\n",
    "            context.logger.info(f'Creating directory {_dir}')\n",
    "            os.makedirs(_dir, exist_ok=True)\n",
    "\n",
    "        # move the files to their appropriate folders (<TRAIN/TEST>/<label>/<file>)\n",
    "        for label, filenames in files.items():\n",
    "            context.logger.info(f'Moving \"{label}\" files in {data_train_test_type}')\n",
    "            for i, file in enumerate(filenames):\n",
    "                shutil.move(file, os.path.join(target_path, data_train_test_type, label, os.path.basename(file)))\n",
    "    shutil.rmtree(extraction_path)\n",
    "\n",
    "    # Add function logging\n",
    "    context.logger.info(f'extracted archive to {target_path}')\n",
    "    context.logger.info(f'Dataset contains the labels {labels}')\n",
    "    \n",
    "    # Log the dataset folder as `content` artifact for later use\n",
    "    context.log_artifact('content', target_path=target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images from s3 using the local `open_archive` function\n",
    "from mlrun import NewTask, run_local\n",
    "import os\n",
    "\n",
    "# Set the source-data URL\n",
    "url_prefix = os.environ.get('DATA_SOURCE_URL_PREFIX', '/v3io/projects/waste-classifier/')\n",
    "\n",
    "# open_archive_task = NewTask(name='download', \n",
    "#                             handler=open_archive, \n",
    "#                             params={'target_path': f'{url_prefix.rstrip(\"/\")}/images'},\n",
    "#                             inputs={'archive_url': f'{url_prefix.rstrip(\"/\")}/waste_dataset.zip'})\n",
    "\n",
    "\n",
    "# download_run = run_local(open_archive_task, \n",
    "#                          project=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project, code_to_function\n",
    "project_dir = './'\n",
    "hvdproj = new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = code_to_function(kind='job', \n",
    "                         name='utils',\n",
    "                         image='mlrun/mlrun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f815f6ba9d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvdproj.set_function(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/User/waste-classifier/src-tf/horovod-training.py\n"
     ]
    }
   ],
   "source": [
    "HOROVOD_FILE = os.path.join(code_dir, 'horovod-training.py')\n",
    "print(HOROVOD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.mpijob.v1.MpiRuntimeV1 at 0x7f815e0b2b10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import new_function\n",
    "import os\n",
    "\n",
    "# Set `use_gpu` to True to run the function using a GPU Configuration\n",
    "use_gpu = False\n",
    "# use_gpu = True\n",
    "\n",
    "image = lambda gpu: 'mlrun/ml-models-gpu' if gpu else 'mlrun/ml-models' \n",
    "\n",
    "# Set basic function parameters\n",
    "trainer = new_function(name='trainer',\n",
    "                       kind='mpijob',\n",
    "                       command=HOROVOD_FILE)\n",
    "trainer.spec.replicas = 2\n",
    "\n",
    "# Set a minimal number of dedicated CPUs per node\n",
    "trainer.with_requests(cpu=4)\n",
    "\n",
    "# Pick image by wanted TF version\n",
    "trainer.spec.image = image(use_gpu)\n",
    "    \n",
    "# Add GPUs to workers?\n",
    "if use_gpu:\n",
    "    trainer.gpus(1)\n",
    "\n",
    "# Registre the function to the project\n",
    "hvdproj.set_function(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f815de667d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvdproj.set_function('hub://tf2_serving', 'serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.artifacts.base.Artifact at 0x7f815de62ad0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvdproj.log_artifact(\n",
    "    'images', \n",
    "    target_path=f'{url_prefix}waste_dataset.zip',\n",
    "    artifact_path=mlconf.artifact_path)\n",
    "#print(hvdproj.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    '''\n",
    "    This function will run before running the project.\n",
    "    It allows us to add our specific system configurations to the functions\n",
    "    like mounts or secrets if needed.\n",
    "\n",
    "    In this case we will add Iguazio's user mount to our functions using the\n",
    "    `mount_v3io()` function to automatically set the mount with the needed\n",
    "    variables taken from the environment. \n",
    "    * mount_v3io can be replaced with mlrun.platforms.mount_pvc() for \n",
    "    non-iguazio mount\n",
    "\n",
    "    @param functions: <function_name: function_yaml> dict of functions in the\n",
    "                        workflow\n",
    "    @param project: project object\n",
    "    @param secrets: secrets required for the functions for s3 connections and\n",
    "                    such\n",
    "    '''\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())                  # On Iguazio (Auto-mount /User)\n",
    "        # f.apply(mlrun.platforms.mount_pvc()) # Non-Iguazio mount\n",
    "        \n",
    "    functions['serving'].set_env('MODEL_CLASS', 'TFModel')\n",
    "    functions['serving'].set_env('IMAGE_HEIGHT', '224')\n",
    "    functions['serving'].set_env('IMAGE_WIDTH', '224')\n",
    "    functions['serving'].set_env('ENABLE_EXPLAINER', 'False')\n",
    "    functions['serving'].spec.min_replicas = 1\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Waste Image classification',\n",
    "    description='Train an Image Classification TF Algorithm using MLRun on Waste Dataset'\n",
    ")\n",
    "def kfpipeline(\n",
    "        image_archive='store:///images',\n",
    "        images_dir=f'/v3io/projects/waste-classifier/images',\n",
    "        checkpoints_dir=f'/v3io/projects/waste-classifier/models/checkpoints',\n",
    "        model_name='waste_classifier',\n",
    "        epochs: int=2):\n",
    "\n",
    "    base_url='/v3io/projects/waste-classifier/images/'\n",
    "    # step 1: download and prep images\n",
    "#     open_archive = funcs['utils'].as_step(name='download',\n",
    "#                                           handler='open_archive',\n",
    "#                                           params={'target_path': images_dir},\n",
    "#                                           inputs={'archive_url': image_archive},\n",
    "#                                           outputs=['content'])\n",
    "\n",
    "    # step 2: train the model\n",
    "#     train_dir = str(open_archive.outputs['content']) + '/TRAIN'\n",
    "#     val_dir = str(open_archive.outputs['content']) + '/TEST'\n",
    "\n",
    "    train_dir = '/v3io/projects/waste-classifier/images/TRAIN'\n",
    "    val_dir = '/v3io/projects/waste-classifier/images/TEST'\n",
    "    train = funcs['trainer'].as_step(name='train',\n",
    "                                     params={'epochs': epochs,\n",
    "                                             'checkpoints_dir': checkpoints_dir,\n",
    "                                             'model_dir'     : 'tfmodels',\n",
    "                                             'train_path'     : train_dir,\n",
    "                                             'val_path'       : val_dir,\n",
    "                                             'batch_size'     : 32},\n",
    "                                     outputs=['model'])\n",
    "\n",
    "    # deploy the model using nuclio functions\n",
    "    deploy = funcs['serving'].deploy_step(models={model_name: train.outputs['model']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.set_workflow('main', 'workflow.py', embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.mlops5.iguazio-c0.com/pipelines/#/experiments/details/a30989d4-abc9-4999-bd87-83eed3b1f28b\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.mlops5.iguazio-c0.com/pipelines/#/runs/details/a480258a-ce66-4232-8094-c3285a4ef8c1\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-07-05 01:45:46,283 [info] Pipeline run id=a480258a-ce66-4232-8094-c3285a4ef8c1, check UI or DB for progress\n",
      "> 2021-07-05 01:45:46,284 [info] waiting for pipeline run completion\n"
     ]
    }
   ],
   "source": [
    "artifact_path = path.abspath(f'{url_prefix}/pipe/{{workflow.uid}}')\n",
    "run_id = hvdproj.run(\n",
    "    'main',\n",
    "    arguments={'model_name': 'waste_classifier',\n",
    "               'images_dir': artifact_path + '/images',\n",
    "              }, \n",
    "    artifact_path=artifact_path, \n",
    "    dirty=True, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test image to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing event\n",
    "organic_image_url = f'{url_prefix}images/TEST/O/O_12568.jpg'\n",
    "\n",
    "if 'http' in organic_image_url:\n",
    "    response = requests.get(organic_image_url)\n",
    "    organic_image = response.content\n",
    "    img = Image.open(BytesIO(organic_image))\n",
    "else:\n",
    "    organic_image = open(organic_image_url,'rb').read()\n",
    "    img = Image.open(organic_image_url)\n",
    "\n",
    "print('Test image:')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing event\n",
    "recyclable_image_url = f'{url_prefix}images/TEST/R/R_10001.jpg'\n",
    "\n",
    "if 'http' in recyclable_image_url:\n",
    "    response = requests.get(recyclable_image_url)\n",
    "    recyclable_image = response.content\n",
    "    img = Image.open(BytesIO(recyclable_image_url))\n",
    "else:\n",
    "    recyclable_image = open(recyclable_image_url,'rb').read()\n",
    "    img = Image.open(recyclable_image)\n",
    "\n",
    "print('Test image:')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send image to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = 'http://nuclio-{}-{}:8080'.format(hvdproj.name, hvdproj.func('serving').metadata.name)\n",
    "print(addr)\n",
    "headers = {'Content-type': 'image/jpeg'}\n",
    "url = addr + f'/waste_classifier/predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Organic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url=url, \n",
    "                         data=json.dumps({'data_url': organic_image_url}), \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "requests.post(url=url, \n",
    "              data=json.dumps({'data_url': organic_image_url}), \n",
    "              headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=url, \n",
    "                         data=organic_image, \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "requests.post(url=url, \n",
    "              data=organic_image, \n",
    "              headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Recyclable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url=url, \n",
    "                         data=json.dumps({'data_url': recyclable_image_url}), \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "requests.post(url=url, \n",
    "              data=json.dumps({'data_url': recyclable_image_url}), \n",
    "              headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=url, \n",
    "                         data=recyclable_image, \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "requests.post(url=url, \n",
    "              data=recyclable_image, \n",
    "              headers=headers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
